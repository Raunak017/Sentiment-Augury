{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Phrase  Sentiment\n",
      "0                      just plain boring          0\n",
      "1  entirely predictable and lacks energy          0\n",
      "2       no surprises and very few laughs          0\n",
      "3                          very powerful          1\n",
      "4        the most fun film of the summer          1\n"
     ]
    }
   ],
   "source": [
    "# Loading Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = pd.read_csv('./sampledata.csv', delimiter=',')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating counts from text using a vectorizer. \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(dataset['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data 70%-25% into training set and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, dataset['Sentiment'], test_size=0.25, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels:  [1 1]\n",
      "Actual Labels:     4    1\n",
      "0    0\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Training Model on the Training set\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Predicted Labels: ',y_pred)\n",
    "print('Actual Labels:    ',Y_test)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Phrase\n",
      "0  predictable with no fun\n"
     ]
    }
   ],
   "source": [
    "# Loading Testing Data\n",
    "\n",
    "testdata = pd.read_csv('./testdata.csv', delimiter=',')\n",
    "print(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Test Data: [0]\n"
     ]
    }
   ],
   "source": [
    "# Generating counts from text using a vectorizer. \n",
    "\n",
    "text_counts1 = cv.transform(testdata['Phrase'])\n",
    "\n",
    "# Predicting the Outcome of the Test Data from the model trained\n",
    "\n",
    "prediction = model.predict(text_counts1)\n",
    "print('Prediction for Test Data:',prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[0 1]\n",
      " [0 1]]\n",
      "Accuacy Score:     0.5\n",
      "Precision Score:   0.25\n",
      "Recall Score:      0.5\n",
      "F-Score:           0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1321: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  % (pos_label, average), UserWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix and Other Metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "print('Confusion Matrix: \\n',cm)\n",
    "print('Accuacy Score:    ',accuracy_score(Y_test, y_pred))\n",
    "print('Precision Score:  ',precision_score(Y_test, y_pred, pos_label='positive',average='macro'))\n",
    "print('Recall Score:     ',recall_score(Y_test, y_pred, pos_label='positive',average='macro'))\n",
    "print('F-Score:          ',f1_score(Y_test, y_pred, pos_label='positive',average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
